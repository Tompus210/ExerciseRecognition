{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a110f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aec2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained classifier\n",
    "model = joblib.load(\"exercise_classifier.pkl\")\n",
    "\n",
    "# Define class labels\n",
    "class_map = {0: \"Pushup\", 1: \"Squat\", 2: \"Pullup\"}\n",
    "\n",
    "# Load YOLO pose model\n",
    "pose_model = YOLO(\"yolo11n-pose.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b0b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video\n",
    "video_path = \"videos/snapshots/Snapshot_video1.mov\"\n",
    "#video_path = \"videos/single_pushup_videos/mirrored_single_pushup_random2.mov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf9bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature functions ===\n",
    "def angle_between_points(p1, p2, p3):\n",
    "    a = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "    b = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "    if np.any(np.isnan(a)) or np.any(np.isnan(b)) or np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
    "        return np.nan\n",
    "    cos_angle = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    if np.any(np.isnan(p1)) or np.any(np.isnan(p2)):\n",
    "        return np.nan\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "def compute_joint_features(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        kp = lambda i: (row[f\"kp_{i}_x\"], row[f\"kp_{i}_y\"])  # works with named columns\n",
    "        feat = {\n",
    "            \"feat_elbow_angle_L\": angle_between_points(kp(5), kp(7), kp(9)),\n",
    "            \"feat_elbow_angle_R\": angle_between_points(kp(6), kp(8), kp(10)),\n",
    "            \"feat_knee_angle_L\": angle_between_points(kp(11), kp(13), kp(15)),\n",
    "            \"feat_knee_angle_R\": angle_between_points(kp(12), kp(14), kp(16)),\n",
    "            \"feat_hip_angle_L\": angle_between_points(kp(5), kp(11), kp(13)),\n",
    "            \"feat_hip_angle_R\": angle_between_points(kp(6), kp(12), kp(14)),\n",
    "            \"feat_shoulder_width\": euclidean_distance(kp(5), kp(6)),\n",
    "            \"feat_hip_to_wrist_L\": euclidean_distance(kp(11), kp(9)),\n",
    "            \"feat_hip_to_wrist_R\": euclidean_distance(kp(12), kp(10)),\n",
    "            \"feat_knee_to_ankle_L\": euclidean_distance(kp(13), kp(15)),\n",
    "            \"feat_knee_to_ankle_R\": euclidean_distance(kp(14), kp(16)),\n",
    "            \"feat_hip_y\": row[\"kp_11_y\"],\n",
    "        }\n",
    "        features.append(feat)\n",
    "    return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded feature names: ['kp_0_x_f0', 'kp_0_y_f0', 'kp_1_x_f0', 'kp_1_y_f0', 'kp_2_x_f0']\n",
      "‚úÖ Combined data columns: Index(['kp_0_x_f0', 'kp_1_x_f0', 'kp_2_x_f0', 'kp_3_x_f0', 'kp_4_x_f0'], dtype='object')\n",
      "üîç Prediction Confidence per Class:\n",
      " - Pushup: 36%\n",
      " - Squat: 33%\n",
      " - Pullup: 31%\n",
      "\n",
      "‚ö†Ô∏è No defined exercise detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tompusch/Library/Mobile Documents/com~apple~CloudDocs/MCI_2/AI project Hollaus/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === Load video ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "cap.release()\n",
    "\n",
    "if len(frames) < 6:\n",
    "    print(\"‚ö†Ô∏è Not enough frames.\")\n",
    "    exit()\n",
    "\n",
    "# === Select 6 equally spaced frames from the video ===\n",
    "sample_idxs = np.linspace(0, len(frames) - 1, min(6, len(frames)), dtype=int)\n",
    "sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "# === Collect keypoints and engineered features with frame suffixes (_f0, _f1, ...) ===\n",
    "keypoint_data = []\n",
    "feature_data = []\n",
    "\n",
    "for i, idx in enumerate(sample_idxs):\n",
    "    results = pose_model.predict(source=frames[idx], conf=0.25, save=False, verbose=False)\n",
    "    keypoints_flat = [np.nan] * (17 * 3)\n",
    "\n",
    "    try:\n",
    "        result = results[0]\n",
    "        if result.keypoints is not None:\n",
    "            keypoints = result.keypoints.xy[0].cpu().numpy()\n",
    "            confidences = result.keypoints.conf[0].cpu().numpy()\n",
    "\n",
    "            flat = []\n",
    "            for j in range(17):\n",
    "                x = y = c = np.nan\n",
    "                if j < len(confidences) and confidences[j] >= 0.2:\n",
    "                    x, y = keypoints[j]\n",
    "                    c = confidences[j]\n",
    "                flat.extend([x, y, c])\n",
    "            keypoints_flat = flat\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Pose estimation failed at frame {idx}: {e}\")\n",
    "\n",
    "    # Create column names with frame suffix (_f0, _f1, etc.)\n",
    "    xy_headers = [f\"kp_{k}_x_f{i}\" for k in range(17)] + [f\"kp_{k}_y_f{i}\" for k in range(17)]\n",
    "    conf_headers = [f\"kp_{k}_conf_f{i}\" for k in range(17)]\n",
    "    df_kp = pd.DataFrame([keypoints_flat], columns=xy_headers + conf_headers)\n",
    "\n",
    "    # Compute joint features and apply frame suffix to columns\n",
    "    df_simple = df_kp.copy()\n",
    "    df_simple.columns = [col.replace(f\"_f{i}\", \"\") for col in df_kp.columns]  # for compute_joint_features\n",
    "    df_feat = compute_joint_features(df_simple)\n",
    "    df_feat.columns = [f\"{col}_f{i}\" for col in df_feat.columns]\n",
    "\n",
    "    keypoint_data.append(df_kp)\n",
    "    feature_data.append(df_feat)\n",
    "\n",
    "# === Merge all frames into one DataFrame\n",
    "df_all_keypoints = pd.concat(keypoint_data, axis=1)\n",
    "df_all_features = pd.concat(feature_data, axis=1)\n",
    "combined_all = pd.concat([df_all_keypoints, df_all_features], axis=1)\n",
    "\n",
    "# === Load the used training feature columns ===\n",
    "if not os.path.exists(\"used_feature_columns.txt\"):\n",
    "    print(\"‚ùå Missing file: used_feature_columns.txt. Run training first.\")\n",
    "    exit()\n",
    "\n",
    "used_columns = pd.read_csv(\"used_feature_columns.txt\", header=None, dtype=str)[0].tolist()\n",
    "assert isinstance(used_columns[0], str), \"‚ùå Feature column names must be strings!\"\n",
    "\n",
    "print(\"‚úÖ Loaded feature names:\", used_columns[:5])\n",
    "print(\"‚úÖ Combined data columns:\", combined_all.columns[:5])\n",
    "\n",
    "# === Ensure all required columns are present ===\n",
    "missing_cols = [col for col in used_columns if col not in combined_all.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Missing expected columns in input: {missing_cols}\")\n",
    "    exit()\n",
    "\n",
    "combined = combined_all[used_columns]\n",
    "\n",
    "# === Impute missing values and make prediction ===\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_sample = imputer.fit_transform(combined).flatten().reshape(1, -1)\n",
    "\n",
    "# === Check if feature count matches model expectation ===\n",
    "if X_sample.shape[1] != model.n_features_in_:\n",
    "    print(f\"‚ùå Feature mismatch: model expects {model.n_features_in_}, but got {X_sample.shape[1]}\")\n",
    "    exit()\n",
    "\n",
    "# === Predict class and probability ===\n",
    "proba = model.predict_proba(X_sample)[0]\n",
    "predicted_class = np.argmax(proba)\n",
    "confidence = round(100 * proba[predicted_class])\n",
    "\n",
    "# === Output results ===\n",
    "print(\"üîç Prediction Confidence per Class:\")\n",
    "for i, cls in class_map.items():\n",
    "    print(f\" - {cls}: {round(100 * proba[i])}%\")\n",
    "\n",
    "if confidence >= 40:\n",
    "    print(f\"\\n‚úÖ Final Prediction: {class_map[predicted_class]} ({confidence}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No defined exercise detected\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
