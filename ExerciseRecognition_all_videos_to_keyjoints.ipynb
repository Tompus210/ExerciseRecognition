{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxNB9ZqSRVHN"
   },
   "source": [
    "kurze Projektbeschreibung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122405,
     "status": "ok",
     "timestamp": 1747734204474,
     "user": {
      "displayName": "Tom Pusch",
      "userId": "07673425930198767409"
     },
     "user_tz": -120
    },
    "id": "hIHHuYShSpbN",
    "outputId": "54ae06fe-e60b-436c-9633-2d97d2fe41ad"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747734335921,
     "user": {
      "displayName": "Tom Pusch",
      "userId": "07673425930198767409"
     },
     "user_tz": -120
    },
    "id": "7GA1iwbPRar8"
   },
   "outputs": [],
   "source": [
    "# Load YOLO pose model\n",
    "model_pose = YOLO('yolo11n-pose.pt')\n",
    "\n",
    "\n",
    "# Define training exercise and corresponding input and output folders\n",
    "\n",
    "#input_folder = \"videos/single_pushup_videos/\"\n",
    "#output_folder = \"keyjoints/pushups/\"\n",
    "\n",
    "#input_folder = \"videos/single_squat_videos/\"\n",
    "#output_folder = \"keyjoints/squats/\"\n",
    "\n",
    "input_folder = \"videos/single_pullup_videos/\"\n",
    "output_folder = \"keyjoints/pullups/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem input Ordner werden single videos geladen und alle keypoints pro video in einer csv gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1V4fUC4MHdAli4G1tVCCjzlJEwXaJIYaB"
    },
    "executionInfo": {
     "elapsed": 12551,
     "status": "ok",
     "timestamp": 1747734350058,
     "user": {
      "displayName": "Tom Pusch",
      "userId": "07673425930198767409"
     },
     "user_tz": -120
    },
    "id": "bAUsjCX_GULa",
    "outputId": "79b37aca-8208-44cd-bd61-cdaa71a3e904"
   },
   "outputs": [],
   "source": [
    "# Process first all videos in the folder\n",
    "video_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.mp4', '.mov'))] # add [:3] if only first three videos\n",
    "\n",
    "for video_file in video_files:\n",
    "    output_csv_path = os.path.join(output_folder, os.path.splitext(video_file)[0] + \".csv\")\n",
    "    if os.path.exists(output_csv_path):\n",
    "        print(f\"⏭️ Skipping {video_file} (CSV already exists)\")\n",
    "        continue\n",
    "    video_path = os.path.join(input_folder, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    output_rows = []\n",
    "    xy_headers = [f\"kp_{i}_{coord}\" for i in range(17) for coord in (\"x\", \"y\")]\n",
    "    conf_headers = [f\"kp_{i}_conf\" for i in range(17)]\n",
    "    columns = [\"frame\", \"time_sec\"] + xy_headers + conf_headers\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        timestamp = frame_idx / fps\n",
    "        results = model_pose.predict(source=frame, save=False, conf=0.25, verbose=False)\n",
    "\n",
    "        for result in results:\n",
    "            keypoints_xy = [np.nan] * (17 * 2)\n",
    "            confidences = [np.nan] * 17\n",
    "\n",
    "            try:\n",
    "                keypoints = result.keypoints.xy[0].cpu().numpy()\n",
    "                confidences = result.keypoints.conf[0].cpu().numpy()\n",
    "\n",
    "                flattened_xy = []\n",
    "                for i in range(17):\n",
    "                    if i >= len(confidences) or confidences[i] < 0.2:\n",
    "                        x, y = np.nan, np.nan\n",
    "                    else:\n",
    "                        x, y = keypoints[i]\n",
    "                    flattened_xy.extend([x, y])\n",
    "            except:\n",
    "                flattened_xy = [np.nan] * (17 * 2)\n",
    "\n",
    "            output_rows.append([frame_idx, timestamp] + flattened_xy + confidences.tolist())\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Save to CSV\n",
    "    df_out = pd.DataFrame(output_rows, columns=columns)\n",
    "    keypoint_cols = [col for col in df_out.columns if col.startswith(\"kp_\")]\n",
    "    df_out[keypoint_cols] = df_out[keypoint_cols].replace(0.0, np.nan)\n",
    "\n",
    "    output_csv_path = os.path.join(output_folder, os.path.splitext(video_file)[0] + \".csv\")\n",
    "    df_out.to_csv(output_csv_path, index=False, na_rep=\"NaN\")\n",
    "    print(f\"✅ Saved: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling\n",
    "\n",
    "Iterate through all csv files from the single exercise videos and extract sampled csv files with only six frames describing an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_2.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_3.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_1.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_4.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_5.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_7.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_6.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_3.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_2.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_1.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_5.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_4.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random9.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_6.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_7.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random8.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random5.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random4.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random6.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_9.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_8.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random7.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random3.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random2.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_random1.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_tom1.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_8.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_tom2.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi2_9.csv\n",
      "✅ Saved: keyjoints_sampled/pushups_sampled/single_pushup_fabi1_10.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_4.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_5.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_7.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas1_5.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom1_4.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom1_5.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas1_4.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_6.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_7.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom1_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_6.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas1_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas3_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas1_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_4.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom1_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom1_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_5.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas1_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi3_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_7.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_6.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi3_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi1_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi1_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi3_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_4.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_5.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi1_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom2_1.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_10.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_3.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_tom2_2.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_8.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_jonas2_9.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_8.csv\n",
      "✅ Saved: keyjoints_sampled/squats_sampled/single_squat_fabi2_9.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to sample 6 evenly spaced frames from full CSV\n",
    "def sample_csv(input_csv_path, output_csv_path, num_samples=6):\n",
    "    if os.path.exists(output_csv_path):\n",
    "        print(f\"⏭️ Skipping {output_csv_path} (already exists)\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    total_frames = len(df)\n",
    "\n",
    "    if total_frames < num_samples:\n",
    "        print(f\"⚠️ Not enough frames in {input_csv_path} ({total_frames} < {num_samples}) — skipped.\")\n",
    "        return\n",
    "\n",
    "    sampled_idxs = np.linspace(0, total_frames - 1, num_samples, dtype=int)\n",
    "    df_sampled = df.iloc[sampled_idxs]\n",
    "    df_sampled.to_csv(output_csv_path, index=False)\n",
    "    print(f\"✅ Saved: {output_csv_path}\")\n",
    "\n",
    "# Input and output folders based on your project structure\n",
    "base_input_dir = \"keyjoints\"\n",
    "base_output_dir = \"keyjoints_sampled\"\n",
    "exercise_types = [\"pushups\", \"squats\", \"pullups\"]\n",
    "\n",
    "# Loop through each exercise type\n",
    "for exercise in exercise_types:\n",
    "    input_folder = os.path.join(base_input_dir, exercise)\n",
    "    output_folder = os.path.join(base_output_dir, f\"{exercise}_sampled\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_csv = os.path.join(input_folder, csv_file)\n",
    "        output_csv = os.path.join(output_folder, csv_file)\n",
    "        sample_csv(input_csv, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqW0uKENMEV4o/2XzrHRZ0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
